{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I will attempt to tweak the multi-level perceptron model provided in the sample notebook, and report my findings. The list of tweaks are listed as follow (keeping everything else default):<br>\n",
    "\n",
    "1) Fully connected layer output shape: (512, 256)(default) vs (256, 128) and (1024, 512)<br>\n",
    "2) Dropout rate: 0.2(default) vs 0.1 vs 0.5<br>\n",
    "3) 2 hiddenlayer vs 3 hiddenlayers: (512, 256)(default) vs [1024, 512, 256].<br>\n",
    "4) learning rate: 0.1(default) vs 0.3 and 0.1/epochs(time-based decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some useful Python libraries:\n",
    "1. numpy: structural data types\n",
    "2. pandas: data loading and manipulation\n",
    "3. matplotlib, sns: data visualization\n",
    "4. sklearn: data analytics algorithms\n",
    "5. Tensorflow: Deep Learning library\n",
    "6. Keras: Wrapper for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import cifar10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, perform reshape, normalization and one-hot encoding of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of final classes (in cifar10, there are 10 classes)\n",
    "nb_classes = 10\n",
    "nb_epoch = 100\n",
    "batch_size = 128\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
    "X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_acc(i, historyList):\n",
    "    fig = plt.figure()\n",
    "    for index, his in enumerate(historyList):\n",
    "        plt.plot(range(nb_epoch),his.history['acc'],label='training'+str(index))\n",
    "    plt.legend(loc=0)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('training_accuracy')\n",
    "    plt.xlim([1,nb_epoch])\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training Accuracy Comparison\")\n",
    "    plt.show()\n",
    "    fig.savefig('img/'+str(i)+'-training-accuracy.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def plot_val_acc(i, historyList):\n",
    "    fig = plt.figure()\n",
    "    for index, his in enumerate(historyList):\n",
    "        plt.plot(range(nb_epoch),his.history['val_acc'],label='validation'+str(index))\n",
    "    plt.legend(loc=0)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('validation_accuracy')\n",
    "    plt.xlim([1,nb_epoch])\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Validation Accuracy Comparison\")\n",
    "    plt.show()\n",
    "    fig.savefig('img/'+str(i)+'-validation-accuracy.png')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def saveHistory(history, filename):\n",
    "    import json\n",
    "    json.dump(history.history, open('json_history/'+filename+'.json', 'w+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Different Output Shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output shape refers to the number of neurons in each layer. In the sample, there are 4 layers:\n",
    "Input Layer, 2 hidden layers with output shape: 512 and 256 respectively, and an Output Layer with outputshape: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define base model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_shape(shape1, shape2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(shape1, input_shape=X_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(shape2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Default Model (based on the sample notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 2.0508 - acc: 0.2534 - val_loss: 1.8805 - val_acc: 0.3403\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 17s 332us/step - loss: 1.8722 - acc: 0.3338 - val_loss: 1.7928 - val_acc: 0.3709\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 17s 333us/step - loss: 1.8027 - acc: 0.3619 - val_loss: 1.7317 - val_acc: 0.3901\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 1.7557 - acc: 0.3799 - val_loss: 1.6864 - val_acc: 0.4079\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 17s 346us/step - loss: 1.7184 - acc: 0.3927 - val_loss: 1.6523 - val_acc: 0.4235\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 18s 350us/step - loss: 1.6857 - acc: 0.4048 - val_loss: 1.6243 - val_acc: 0.4346\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 18s 350us/step - loss: 1.6584 - acc: 0.4139 - val_loss: 1.6143 - val_acc: 0.4354\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 18s 364us/step - loss: 1.6329 - acc: 0.4234 - val_loss: 1.5732 - val_acc: 0.4509\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 16s 310us/step - loss: 1.6124 - acc: 0.4314 - val_loss: 1.5670 - val_acc: 0.4507\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 15s 306us/step - loss: 1.5949 - acc: 0.4383 - val_loss: 1.5755 - val_acc: 0.4464\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 15s 304us/step - loss: 1.5763 - acc: 0.4426 - val_loss: 1.5477 - val_acc: 0.4523\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 17s 339us/step - loss: 1.5598 - acc: 0.4497 - val_loss: 1.5146 - val_acc: 0.4724\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 17s 334us/step - loss: 1.5412 - acc: 0.4575 - val_loss: 1.5079 - val_acc: 0.4709\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 19s 372us/step - loss: 1.5322 - acc: 0.4616 - val_loss: 1.5045 - val_acc: 0.4747\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 16s 321us/step - loss: 1.5166 - acc: 0.4647 - val_loss: 1.4689 - val_acc: 0.4879\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 1.5020 - acc: 0.4685 - val_loss: 1.4873 - val_acc: 0.4739\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 16s 326us/step - loss: 1.4946 - acc: 0.4746 - val_loss: 1.4835 - val_acc: 0.4794\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 1.4826 - acc: 0.4782 - val_loss: 1.4480 - val_acc: 0.4926\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 17s 344us/step - loss: 1.4710 - acc: 0.4811 - val_loss: 1.4770 - val_acc: 0.4798\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 1.4610 - acc: 0.4846 - val_loss: 1.4581 - val_acc: 0.4857\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 18s 357us/step - loss: 1.4433 - acc: 0.4913 - val_loss: 1.4337 - val_acc: 0.4949\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 19s 382us/step - loss: 1.4402 - acc: 0.4927 - val_loss: 1.4967 - val_acc: 0.4839\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 17s 334us/step - loss: 1.4289 - acc: 0.4993 - val_loss: 1.4091 - val_acc: 0.5042\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 16s 328us/step - loss: 1.4187 - acc: 0.5003 - val_loss: 1.4388 - val_acc: 0.4895\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 1.4117 - acc: 0.5037 - val_loss: 1.3985 - val_acc: 0.5134\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 17s 340us/step - loss: 1.4011 - acc: 0.5055 - val_loss: 1.4051 - val_acc: 0.5055\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 1.3947 - acc: 0.5088 - val_loss: 1.3891 - val_acc: 0.5102\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 17s 337us/step - loss: 1.3864 - acc: 0.5122 - val_loss: 1.3905 - val_acc: 0.5163\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 16s 322us/step - loss: 1.3760 - acc: 0.5140 - val_loss: 1.3688 - val_acc: 0.5230\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 16s 320us/step - loss: 1.3699 - acc: 0.5159 - val_loss: 1.3946 - val_acc: 0.5057\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 17s 334us/step - loss: 1.3621 - acc: 0.5193 - val_loss: 1.3894 - val_acc: 0.5171\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 16s 318us/step - loss: 1.3563 - acc: 0.5221 - val_loss: 1.4199 - val_acc: 0.4943\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 16s 317us/step - loss: 1.3528 - acc: 0.5222 - val_loss: 1.3670 - val_acc: 0.5161\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 15s 310us/step - loss: 1.3421 - acc: 0.5268 - val_loss: 1.4149 - val_acc: 0.5002\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 16s 316us/step - loss: 1.3414 - acc: 0.5271 - val_loss: 1.3742 - val_acc: 0.5127\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 16s 315us/step - loss: 1.3254 - acc: 0.5331 - val_loss: 1.3504 - val_acc: 0.5225\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 16s 315us/step - loss: 1.3175 - acc: 0.5369 - val_loss: 1.3392 - val_acc: 0.5291\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 16s 312us/step - loss: 1.3122 - acc: 0.5376 - val_loss: 1.3511 - val_acc: 0.5189\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 16s 315us/step - loss: 1.3085 - acc: 0.5398 - val_loss: 1.3498 - val_acc: 0.5294\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 16s 317us/step - loss: 1.2980 - acc: 0.5409 - val_loss: 1.3816 - val_acc: 0.5151\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 16s 314us/step - loss: 1.2947 - acc: 0.5441 - val_loss: 1.3255 - val_acc: 0.5364\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 15s 309us/step - loss: 1.2854 - acc: 0.5458 - val_loss: 1.3536 - val_acc: 0.5221\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 16s 320us/step - loss: 1.2816 - acc: 0.5472 - val_loss: 1.3874 - val_acc: 0.5061\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 16s 316us/step - loss: 1.2735 - acc: 0.5511 - val_loss: 1.3420 - val_acc: 0.5293\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 16s 317us/step - loss: 1.2662 - acc: 0.5507 - val_loss: 1.3236 - val_acc: 0.5319\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 17s 334us/step - loss: 1.2615 - acc: 0.5526 - val_loss: 1.3316 - val_acc: 0.5317\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 16s 316us/step - loss: 1.2575 - acc: 0.5575 - val_loss: 1.3392 - val_acc: 0.5270\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 16s 317us/step - loss: 1.2524 - acc: 0.5580 - val_loss: 1.3209 - val_acc: 0.5371\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 1.2454 - acc: 0.5609 - val_loss: 1.3125 - val_acc: 0.5367\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 1.2425 - acc: 0.5604 - val_loss: 1.3137 - val_acc: 0.5414\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 16s 328us/step - loss: 1.2311 - acc: 0.5658 - val_loss: 1.3426 - val_acc: 0.5276\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 17s 340us/step - loss: 1.2281 - acc: 0.5655 - val_loss: 1.3489 - val_acc: 0.5180\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 17s 334us/step - loss: 1.2222 - acc: 0.5673 - val_loss: 1.2932 - val_acc: 0.5489\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 1.2192 - acc: 0.5674 - val_loss: 1.2996 - val_acc: 0.5435\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 20s 398us/step - loss: 1.2166 - acc: 0.5719 - val_loss: 1.3045 - val_acc: 0.5418\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 18s 368us/step - loss: 1.2032 - acc: 0.5722 - val_loss: 1.2957 - val_acc: 0.5463\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 18s 359us/step - loss: 1.2022 - acc: 0.5742 - val_loss: 1.3147 - val_acc: 0.5393\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 17s 349us/step - loss: 1.1939 - acc: 0.5797 - val_loss: 1.3245 - val_acc: 0.5344\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 17s 330us/step - loss: 1.1915 - acc: 0.5786 - val_loss: 1.2967 - val_acc: 0.5401\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 17s 334us/step - loss: 1.1855 - acc: 0.5827 - val_loss: 1.3083 - val_acc: 0.5447\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 17s 330us/step - loss: 1.1814 - acc: 0.5836 - val_loss: 1.2905 - val_acc: 0.5425\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 16s 328us/step - loss: 1.1756 - acc: 0.5853 - val_loss: 1.3295 - val_acc: 0.5249\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 16s 327us/step - loss: 1.1706 - acc: 0.5840 - val_loss: 1.2873 - val_acc: 0.5420\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 16s 329us/step - loss: 1.1669 - acc: 0.5886 - val_loss: 1.3309 - val_acc: 0.5191\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 16s 329us/step - loss: 1.1622 - acc: 0.5892 - val_loss: 1.3462 - val_acc: 0.5232\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 16s 329us/step - loss: 1.1547 - acc: 0.5901 - val_loss: 1.3364 - val_acc: 0.5289\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 17s 332us/step - loss: 1.1497 - acc: 0.5957 - val_loss: 1.2884 - val_acc: 0.5463\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 17s 337us/step - loss: 1.1487 - acc: 0.5921 - val_loss: 1.3107 - val_acc: 0.5405\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 1.1399 - acc: 0.5970 - val_loss: 1.2803 - val_acc: 0.5545\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 17s 332us/step - loss: 1.1373 - acc: 0.5983 - val_loss: 1.2906 - val_acc: 0.5450\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 17s 332us/step - loss: 1.1343 - acc: 0.5987 - val_loss: 1.2725 - val_acc: 0.5487\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 17s 332us/step - loss: 1.1280 - acc: 0.5999 - val_loss: 1.3031 - val_acc: 0.5430\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 17s 339us/step - loss: 1.1237 - acc: 0.6027 - val_loss: 1.2823 - val_acc: 0.5467\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 1.1215 - acc: 0.6009 - val_loss: 1.2856 - val_acc: 0.5429\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 1.1153 - acc: 0.6027 - val_loss: 1.2863 - val_acc: 0.5487\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 17s 333us/step - loss: 1.1081 - acc: 0.6086 - val_loss: 1.2780 - val_acc: 0.5502\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 17s 333us/step - loss: 1.1057 - acc: 0.6093 - val_loss: 1.2690 - val_acc: 0.5526\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 17s 342us/step - loss: 1.1030 - acc: 0.6089 - val_loss: 1.2600 - val_acc: 0.5541\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 17s 336us/step - loss: 1.0931 - acc: 0.6113 - val_loss: 1.3468 - val_acc: 0.5279\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 17s 337us/step - loss: 1.0911 - acc: 0.6135 - val_loss: 1.3329 - val_acc: 0.5372\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 1.0905 - acc: 0.6127 - val_loss: 1.2542 - val_acc: 0.5573\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 17s 337us/step - loss: 1.0807 - acc: 0.6174 - val_loss: 1.3105 - val_acc: 0.5369\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 17s 333us/step - loss: 1.0835 - acc: 0.6141 - val_loss: 1.2827 - val_acc: 0.5464\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 17s 336us/step - loss: 1.0724 - acc: 0.6180 - val_loss: 1.2738 - val_acc: 0.5512\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 17s 332us/step - loss: 1.0703 - acc: 0.6205 - val_loss: 1.2987 - val_acc: 0.5439\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 1.0640 - acc: 0.6220 - val_loss: 1.2674 - val_acc: 0.5551\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 1.0577 - acc: 0.6255 - val_loss: 1.2604 - val_acc: 0.5559\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 17s 348us/step - loss: 1.0528 - acc: 0.6267 - val_loss: 1.2903 - val_acc: 0.5497\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 17s 343us/step - loss: 1.0565 - acc: 0.6285 - val_loss: 1.2674 - val_acc: 0.5550\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 1.0506 - acc: 0.6267 - val_loss: 1.3036 - val_acc: 0.5467\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 1.0466 - acc: 0.6283 - val_loss: 1.2953 - val_acc: 0.5430\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 1.0404 - acc: 0.6323 - val_loss: 1.2665 - val_acc: 0.5567\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 17s 339us/step - loss: 1.0362 - acc: 0.6346 - val_loss: 1.2941 - val_acc: 0.5502\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 1.0287 - acc: 0.6358 - val_loss: 1.2961 - val_acc: 0.5496\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 1.0310 - acc: 0.6346 - val_loss: 1.2497 - val_acc: 0.5629\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 17s 346us/step - loss: 1.0197 - acc: 0.6401 - val_loss: 1.2748 - val_acc: 0.5564\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 17s 339us/step - loss: 1.0201 - acc: 0.6371 - val_loss: 1.2585 - val_acc: 0.5588\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 17s 337us/step - loss: 1.0118 - acc: 0.6416 - val_loss: 1.2862 - val_acc: 0.5512\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 17s 337us/step - loss: 1.0146 - acc: 0.6413 - val_loss: 1.2581 - val_acc: 0.5562\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 17s 337us/step - loss: 1.0090 - acc: 0.6429 - val_loss: 1.2872 - val_acc: 0.5576\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ee046af3f366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     validation_data=(X_test, Y_test))\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistoryDef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'historyDef.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "modelDef = MLP_shape(512, 256) #default model\n",
    "historyDef = modelDef.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "jsaveHistory(historyDef,'historyDef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 2 other models with different output shapes and plot the graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MLP_shape(256, 128)\n",
    "model2 = MLP_shape(1024, 512)\n",
    "\n",
    "history1 = model1.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history1,'history1')\n",
    "\n",
    "history2 = model2.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history2,'history2')\n",
    "\n",
    "plot_train_acc(1, [historyDef, history1, history2])\n",
    "plot_val_acc(2, [historyDef, history1, history2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Different dropout rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define base model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_drop_rate(drop_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=X_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 3 different models with different drop rate and plot the graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = MLP_drop_rate(0.1)\n",
    "model4 = MLP_drop_rate(0.5)\n",
    "\n",
    "history3 = model3.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history3,'history3')\n",
    "history4 = model4.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history4,'history4')\n",
    "\n",
    "plot_train_acc(3, [historyDef, history3, history4])\n",
    "plot_val_acc(4, [historyDef, history3, history4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Different number of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 5 has 3 hiden layers\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(1024, input_shape=X_train.shape[1:]))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(Dense(512))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.2)\n",
    "model5.add(Dense(256))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(Dense(10))\n",
    "model5.add(Activation('softmax'))\n",
    "\n",
    "model5.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history5 = model5.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history5,'history5')\n",
    "           \n",
    "plot_train_acc(5 [historyDef, history5])\n",
    "plot_val_acc(6, [historyDef, history5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Different Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define base model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_learningRate(lr, decay):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=X_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    sgd = SGD(lr=lr, momentum=0.0, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = MLP_learningRate(0.3, 0.0)\n",
    "model7 = MLP_learningRate(0.1, 0.1/nb_epoch)\n",
    "\n",
    "history6 = model6.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history6,'history6')\n",
    "history7 = model7.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history7,'history7')\n",
    "\n",
    "plot_train_acc(7, [historyDef, history6, history7])\n",
    "plot_val_acc(8, [historyDef, history6, history7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Different Activation Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define base model function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_act_func(act_func):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=X_train.shape[1:]))\n",
    "    model.add(Activation(act_func))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(act_func))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = MLP_act_func('sigmoid')\n",
    "model9 = MLP_act_func('prelu')\n",
    "model10 = MLP_act_func('softplus')\n",
    "\n",
    "history8 = model8.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history8,'history8')\n",
    "history9 = model9.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history9,'history9')\n",
    "history10 = model10.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history10,'history10')\n",
    "\n",
    "plot_train_acc(7, [historyDef, history8, history9, history10])\n",
    "plot_val_acc(8, [historyDef, history8, history9, history10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Different Loss Function:\n",
    "Since there are 10 classes in this dataset, 'categorical cross entropy' is the only option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_loss_func(loss_func):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=X_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=loss_func,\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = MLP_loss_func('sparse_categorical_crossentropy')\n",
    "\n",
    "\n",
    "history11 = model11.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "saveHistory(history11,'history11')\n",
    "\n",
    "plot_train_acc(10, [historyDef, history11])\n",
    "plot_val_acc(12, [historyDef, history11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add your analysis and results using these markdown cells.\n",
    "Some of the things you can play with for your models include\n",
    "1. Effects of varying number of hidden layers\n",
    "2. Effects of varying number of neurons in the hidden layers\n",
    "3. Effects of different learning rates\n",
    "4. Effects of other parameters (training method, activation function, loss function etc)\n",
    "\n",
    "You can also change the metric for classifier performance by using other measures such as mean squared error, confusion matrices etc.\n",
    "\n",
    "Here are some useful links for learning more about creating NN architectures:\n",
    "1. Keras - http://keras.io\n",
    "2. Tensorflow - http://www.tensorflow.org/\n",
    "3. Python numpy tutorial -  http://cs231n.github.io/python-numpy-tutorial/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
